{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Lab 02\n",
    "\n",
    "### Let's code a sentiment classifier on the IMDB sentiment datase\n",
    "\n",
    "---\n",
    "\n",
    "Authors :\n",
    "\n",
    "eliott.bouhana\\\n",
    "victor.simonin\\\n",
    "alexandre.lemonnier\\\n",
    "sarah.gutierez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/bictole/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021706581115722656,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 52,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839e42df1f164970a7276cd426a25fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the datasets splits ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'test', 'unsupervised']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import get_dataset_split_names\n",
    "\n",
    "get_dataset_split_names(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the dataset splits size ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the proportion of each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negatives sentences:  12500\n",
      "Number of positives sentences:  12500\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of negatives sentences: \", dataset[\"train\"][\"label\"].count(0))\n",
    "print(\"Number of positives sentences: \", dataset[\"train\"][\"label\"].count(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Lowercase the text\\\n",
    "Remove punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from typing import List\n",
    "\n",
    "def preprocessor(x_list: List[str]) -> List[str]:\n",
    "    return [x.lower().translate(str.maketrans(\"\", \"\", punctuation)) for x in x_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "A scikit-learn `Pipeline` with a `CountVectorizer` and `MultinomialNB` classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocess\", FunctionTransformer(preprocessor)),\n",
    "    (\"vectorizer\", CountVectorizer(lowercase=True)),\n",
    "    (\"nb\", MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocess',\n",
       "                 FunctionTransformer(func=<function preprocessor at 0x7f430d7e5750>)),\n",
       "                ('vectorizer', CountVectorizer()), ('nb', MultinomialNB())])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(np.array(dataset[\"train\"][\"text\"]), np.array(dataset[\"train\"][\"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Accuracy report on both training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.91284\n",
      "Test accuracy:  0.8172\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Train accuracy: \", accuracy_score(pipeline.predict(np.array(dataset[\"train\"][\"text\"])), np.array(dataset[\"train\"][\"label\"])))\n",
    "print(\"Test accuracy: \", accuracy_score(pipeline.predict(np.array(dataset[\"test\"][\"text\"])), np.array(dataset[\"test\"][\"label\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is accuracy a sufficient measure of evaluation here ?\n",
    "\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Bonus] What are the top 10 most important words (features) for each class?\n",
    "\n",
    "The words with the highest likelihood in each class :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'and', 'of', 'to', 'is', 'in', 'this', 'it', 'that', 'br']\n",
      "['the', 'and', 'of', 'to', 'is', 'in', 'it', 'this', 'that', 'br']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bictole/.local/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "features_likelihood_zero = {}\n",
    "features_likelihood_one = {}\n",
    "features = pipeline.get_params()[\"vectorizer\"].get_feature_names()\n",
    "likelihood_zero = pipeline.get_params()['nb'].feature_log_prob_[0]\n",
    "likelihood_one = pipeline.get_params()['nb'].feature_log_prob_[1]\n",
    "for i, feature in enumerate(features):\n",
    "    features_likelihood_zero[feature] = likelihood_zero[i]\n",
    "    features_likelihood_one[feature] = likelihood_one[i]\n",
    "\n",
    "print(sorted(features_likelihood_zero, key=features_likelihood_zero.get, reverse=True)[:10])\n",
    "print(sorted(features_likelihood_one, key=features_likelihood_one.get, reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the stopwords and checking again :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bictole/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bictole/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['br', 'movie', 'film', 'one', 'like', 'even', 'good', 'bad', 'would', 'really']\n",
      "['br', 'film', 'movie', 'one', 'like', 'good', 'story', 'great', 'time', 'see']\n"
     ]
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "stopWords_zero = []\n",
    "stopWords_one = []\n",
    "for feature in features_likelihood_zero.keys():\n",
    "    if feature in stopWords:\n",
    "        stopWords_zero.append(feature)\n",
    "for feature in features_likelihood_one.keys():\n",
    "    if feature in stopWords:\n",
    "        stopWords_one.append(feature)\n",
    "for stopWord in stopWords_zero:\n",
    "    del features_likelihood_zero[stopWord]\n",
    "for stopWord in stopWords_one:\n",
    "    del features_likelihood_one[stopWord]\n",
    "    \n",
    "print(sorted(features_likelihood_zero, key=features_likelihood_zero.get, reverse=True)[:10])\n",
    "print(sorted(features_likelihood_one, key=features_likelihood_one.get, reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take at least 2 wrongly classified example from the test set and try explaining why the model failed :\n",
    "\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Stemming and Lemmatization\n",
    "\n",
    "Let's add lemmatization to our pretreatment.\n",
    "\n",
    "First, let's demonstrate the lemmatization effect on the first element of our train dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I\n",
      "\n",
      "Original : rented, New: rent\n",
      "Original : AM, New: be\n",
      "Original : surrounded, New: surround\n",
      "Original : was, New: be\n",
      "Original : released, New: release\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "test_list = dataset['train']['text'][0].split()[:25]\n",
    "test_sentence = ' '.join(test_list)\n",
    "\n",
    "doc = spacy_nlp(test_sentence)\n",
    "tokens = [token.text for token in doc]\n",
    "print('Original Sentence: %s' % (test_sentence))\n",
    "print()\n",
    "for token in doc:\n",
    "    if token.text != token.lemma_:\n",
    "        print('Original : %s, New: %s' % (token.text, token.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_preprocessor(x_list: List[str]) -> List[str]:\n",
    "    no_punc_lower = [x.lower().translate(str.maketrans(\"\", \"\", punctuation)) for x in x_list]\n",
    "    spacy_nlp = spacy.load('en_core_web_sm')\n",
    "    return [spacy_nlp(element).text for element in no_punc_lower]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and evaluation of the model again with these pretreatment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocess',\n",
       "                 FunctionTransformer(func=<function lemma_preprocessor at 0x7f4290443e20>)),\n",
       "                ('vectorizer', CountVectorizer()), ('nb', MultinomialNB())])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"preprocess\", FunctionTransformer(lemma_preprocessor)),\n",
    "    (\"vectorizer\", CountVectorizer(lowercase=True)),\n",
    "    (\"nb\", MultinomialNB()),\n",
    "])\n",
    "\n",
    "pipeline.fit(np.array(dataset[\"train\"][\"text\"]), np.array(dataset[\"train\"][\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.91284\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy: \", accuracy_score(pipeline.predict(np.array(dataset[\"train\"][\"text\"])), np.array(dataset[\"train\"][\"label\"])))\n",
    "print(\"Test accuracy: \", accuracy_score(pipeline.predict(np.array(dataset[\"test\"][\"text\"])), np.array(dataset[\"test\"][\"label\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to change the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the results better or worse? Try explaining why the accuracy changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
