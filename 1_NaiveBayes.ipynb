{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Lab 02\n",
    "\n",
    "### Let's code a sentiment classifier on the IMDB sentiment datase\n",
    "\n",
    "---\n",
    "\n",
    "Authors :\n",
    "\n",
    "eliott.bouhana\\\n",
    "victor.simonin\\\n",
    "alexandre.lemonnier\\\n",
    "sarah.gutierez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/leme/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "100%|██████████| 3/3 [00:00<00:00, 290.60it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many splits does the dataset has?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'test', 'unsupervised']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import get_dataset_split_names\n",
    "\n",
    "get_dataset_split_names(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is composed of 3 splits: `train`, `test` and `unsupervised`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How big are these splits ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train` and `test` splits have 25000 rows, whereas `unsupervised` has 50000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the proportion of each class on the supervised splits ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negatives sentences in train supervised split:  12500\n",
      "Number of positives sentences in train supervised split:  12500\n",
      "--------------------------------\n",
      "Number of negatives sentences in test supervised split:  12500\n",
      "Number of positives sentences in test supervised split:  12500\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of negatives sentences in train supervised split: \", dataset[\"train\"][\"label\"].count(0))\n",
    "print(\"Number of positives sentences in train supervised split: \", dataset[\"train\"][\"label\"].count(1))\n",
    "print(\"--------------------------------\")\n",
    "print(\"Number of negatives sentences in test supervised split: \", dataset[\"test\"][\"label\"].count(0))\n",
    "print(\"Number of positives sentences in test supervised split: \", dataset[\"test\"][\"label\"].count(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from typing import List\n",
    "\n",
    "def preprocessor(x_list: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preprocessing function to lowercase and remove punctuation of a list of string.\n",
    "    \n",
    "    Args:\n",
    "        x_list: List of strings\n",
    "    \n",
    "    Returns:\n",
    "        List of preprocessed strings.\n",
    "    \"\"\"\n",
    "    return [x.lower().translate(str.maketrans(\"\", \"\", punctuation)) for x in x_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "A scikit-learn `Pipeline` with a `FunctionTransformer` calling our preprocessing function, a `CountVectorizer` and a `MultinomialNB` classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocess\", FunctionTransformer(preprocessor)),\n",
    "    (\"vectorizer\", CountVectorizer(lowercase=True)),\n",
    "    (\"nb\", MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocess',\n",
       "                 FunctionTransformer(func=<function preprocessor at 0x7f731571e950>)),\n",
       "                ('vectorizer', CountVectorizer()), ('nb', MultinomialNB())])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the pipeline with the train dataset\n",
    "pipeline.fit(np.array(dataset[\"train\"][\"text\"]), np.array(dataset[\"train\"][\"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Accuracy report on both training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.91284\n",
      "Test accuracy:  0.8172\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Evaluate the prediction with train and test datasets\n",
    "print(\"Train accuracy: \", accuracy_score(pipeline.predict(np.array(dataset[\"train\"][\"text\"])), np.array(dataset[\"train\"][\"label\"])))\n",
    "print(\"Test accuracy: \", accuracy_score(pipeline.predict(np.array(dataset[\"test\"][\"text\"])), np.array(dataset[\"test\"][\"label\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is accuracy a sufficient measure of evaluation here ?\n",
    "\n",
    "The accuracy is a sufficient measure because the number of positives and negatives sentences are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Bonus] What are the top 10 most important words (features) for each class?\n",
    "\n",
    "The words with the highest likelihood in each class :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'and', 'of', 'to', 'is', 'in', 'this', 'it', 'that', 'br']\n",
      "['the', 'and', 'of', 'to', 'is', 'in', 'it', 'this', 'that', 'br']\n"
     ]
    }
   ],
   "source": [
    "features_likelihood_zero = {}\n",
    "features_likelihood_one = {}\n",
    "# Get all features (words)\n",
    "features = pipeline.get_params()[\"vectorizer\"].get_feature_names()\n",
    "likelihood_zero = pipeline.get_params()['nb'].feature_log_prob_[0]\n",
    "likelihood_one = pipeline.get_params()['nb'].feature_log_prob_[1]\n",
    "# Assign to words their likelihood probability to be in negative and positive text\n",
    "for i, feature in enumerate(features):\n",
    "    features_likelihood_zero[feature] = likelihood_zero[i]\n",
    "    features_likelihood_one[feature] = likelihood_one[i]\n",
    "\n",
    "print(sorted(features_likelihood_zero, key=features_likelihood_zero.get, reverse=True)[:10])\n",
    "print(sorted(features_likelihood_one, key=features_likelihood_one.get, reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the stopwords and checking again :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/leme/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/leme/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Setup nltk\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['br', 'movie', 'film', 'one', 'like', 'even', 'good', 'bad', 'would', 'really']\n",
      "['br', 'film', 'movie', 'one', 'like', 'good', 'story', 'great', 'time', 'see']\n"
     ]
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "stopWords_zero = []\n",
    "stopWords_one = []\n",
    "# Find stop words in features\n",
    "for feature in features_likelihood_zero.keys():\n",
    "    if feature in stopWords:\n",
    "        stopWords_zero.append(feature)\n",
    "for feature in features_likelihood_one.keys():\n",
    "    if feature in stopWords:\n",
    "        stopWords_one.append(feature)\n",
    "# Remove stop words in dictionaries\n",
    "for stopWord in stopWords_zero:\n",
    "    del features_likelihood_zero[stopWord]\n",
    "for stopWord in stopWords_one:\n",
    "    del features_likelihood_one[stopWord]\n",
    "    \n",
    "print(sorted(features_likelihood_zero, key=features_likelihood_zero.get, reverse=True)[:10])\n",
    "print(sorted(features_likelihood_one, key=features_likelihood_one.get, reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take at least 2 wrongly classified example from the test set and try explaining why the model failed :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blind Date (Columbia Pictures, 1934), was a decent film, but I have a few issues with this film. First of all, I don't fault the actors in this film at all, but more or less, I have a problem with the script. Also, I understand that this film was made in the 1930's and people were looking to escape reality, but the script made Ann Sothern's character look weak. She kept going back and forth between suitors and I felt as though she should have stayed with Paul Kelly's character in the end. He truly did care about her and her family and would have done anything for her and he did by giving her up in the end to fickle Neil Hamilton who in my opinion was only out for a good time. Paul Kelly's character, although a workaholic was a man of integrity and truly loved Kitty (Ann Sothern) as opposed to Neil Hamilton, while he did like her a lot, I didn't see the depth of love that he had for her character. The production values were great, but the script could have used a little work.\n",
      "Prediction: 1\n",
      "Expected: 0\n",
      "\n",
      "Ben, (Rupert Grint), is a deeply unhappy adolescent, the son of his unhappily married parents. His father, (Nicholas Farrell), is a vicar and his mother, (Laura Linney), is ... well, let's just say she's a somewhat hypocritical soldier in Jesus' army. It's only when he takes a summer job as an assistant to a foul-mouthed, eccentric, once-famous and now-forgotten actress Evie Walton, (Julie Walters), that he finally finds himself in true 'Harold and Maude' fashion. Of course, Evie is deeply unhappy herself and it's only when these two sad sacks find each other that they can put their mutual misery aside and hit the road to happiness.<br /><br />Of course it's corny and sentimental and very predictable but it has a hard side to it, too and Walters, who could sleep-walk her way through this sort of thing if she wanted, is excellent. It's when she puts the craziness to one side and finds the pathos in the character, (like hitting the bottle and throwing up in the sink), that she's at her best. The problem is she's the only interesting character in the film (and it's not because of the script which doesn't do anybody any favours). Grint, on the other hand, isn't just unhappy; he's a bit of a bore as well while Linney's starched bitch is completely one-dimensional. (Still, she's got the English accent off pat). The best that can be said for it is that it's mildly enjoyable - with the emphasis on the mildly.\n",
      "Prediction: 1\n",
      "Expected: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Store the prediction result on the test dataset\n",
    "test_predict_res = pipeline.predict(np.array(dataset[\"test\"][\"text\"])), np.array(dataset[\"test\"][\"label\"])\n",
    "wrong_class = {}\n",
    "\n",
    "# Find some wrongly classified text from the test set\n",
    "for i, (got, expected) in enumerate(zip(test_predict_res[0], test_predict_res[1])):\n",
    "    if len(wrong_class) == 2:\n",
    "        break\n",
    "    if got != expected:\n",
    "        wrong_class[dataset[\"test\"][\"text\"][i]] = (got, expected)\n",
    "        \n",
    "# Display the results\n",
    "for text, res in wrong_class.items():\n",
    "    print(text)\n",
    "    print(\"Prediction:\", res[0])\n",
    "    print(\"Expected:\", res[1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two examples are classified as positive sentences instead of negative.\n",
    "\n",
    "For the first example, the reviewer says some positive keywords such as `decent film`, `loved`, `great`, `good`, `like`... that could make the prediction goes wrong.\n",
    "\n",
    "For the second one, there are still some positive keywords such as `excellent`, `enjoyable`, `best`... but the text is still really negative, so we can guess that the length of the review can inferred with the prediction, because this one is pretty long. The presence of some HTML tag may make failed the prediction too.\n",
    "\n",
    "To conclude, this two text may be wrongly classified due to an highest number of words that have more probability to be in the other prediction label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Stemming and Lemmatization\n",
    "### Lemmatization\n",
    "Let's add lemmatization to our pretreatment.\n",
    "\n",
    "First, let's demonstrate the lemmatization effect on the first element of our train dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I\n",
      "\n",
      "Original : rented, New: rent\n",
      "Original : AM, New: be\n",
      "Original : surrounded, New: surround\n",
      "Original : was, New: be\n",
      "Original : released, New: release\n"
     ]
    }
   ],
   "source": [
    "# Setup spacy\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Take a 25 characters sentence example from the test dataset\n",
    "test_list = dataset['train']['text'][0].split()[:25]\n",
    "test_sentence = ' '.join(test_list)\n",
    "\n",
    "# Lemmatize the sentence\n",
    "doc = spacy_nlp(test_sentence)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "print('Original Sentence: %s' % (test_sentence))\n",
    "print()\n",
    "for token in doc:\n",
    "    if token.text != token.lemma_:\n",
    "        print('Original : %s, New: %s' % (token.text, token.lemma_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply lemmatization to the data used in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_preprocessor(x_list: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preprocessing function to lowercase and remove punctuation\n",
    "    of a list of string and lemmatize each string.\n",
    "    \n",
    "    Args:\n",
    "        x_list: List of strings\n",
    "    \n",
    "    Returns:\n",
    "        List of preprocessed strings.\n",
    "    \"\"\"\n",
    "    no_punc_lower = [x.lower().translate(str.maketrans(\"\", \"\", punctuation)) for x in x_list]\n",
    "    spacy_nlp = spacy.load('en_core_web_sm')\n",
    "    res = []\n",
    "    for sentence in no_punc_lower:\n",
    "        doc = spacy_nlp(sentence)\n",
    "        s = []\n",
    "        for word in doc:\n",
    "            s.append(word.lemma_)\n",
    "        s = ' '.join(s)\n",
    "        res.append(s)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and evaluation of the model again with these pretreatment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocess',\n",
       "                 FunctionTransformer(func=<function lemma_preprocessor at 0x7f72d4cea710>)),\n",
       "                ('vectorizer', CountVectorizer()), ('nb', MultinomialNB())])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocess\", FunctionTransformer(lemma_preprocessor)),\n",
    "    (\"vectorizer\", CountVectorizer(lowercase=True)),\n",
    "    (\"nb\", MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Fit the pipeline with train dataset\n",
    "pipeline.fit(np.array(dataset[\"train\"][\"text\"]), np.array(dataset[\"train\"][\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.90624\n",
      "Test accuracy:  0.81356\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the prediction with train and test datasets\n",
    "print(\"Train accuracy: \", accuracy_score(pipeline.predict(np.array(dataset[\"train\"][\"text\"])), np.array(dataset[\"train\"][\"label\"])))\n",
    "print(\"Test accuracy: \", accuracy_score(pipeline.predict(np.array(dataset[\"test\"][\"text\"])), np.array(dataset[\"test\"][\"label\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to change the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the results better or worse? Try explaining why the accuracy changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Let's add stemming to our pretreatment.\n",
    "\n",
    "First, let's demonstrate the stemming effect on the first element of our train dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/leme/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to download a package for word tokenization\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a 25 characters sentence example from the test dataset\n",
    "test_list = dataset['train']['text'][0].split()[:25]\n",
    "test_sentence = ' '.join(test_list)\n",
    "test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I\n",
      "\n",
      "Stemmed sentence: i rent i am from my video store becaus of all the controversi that surround it when it was first releas in i\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re_word = re.compile(r\"^\\w+$\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmed = [stemmer.stem(word) for word in word_tokenize(test_sentence.lower()) if re_word.match(word)]\n",
    "stemmed = ' '.join(stemmed)\n",
    "print(\"Original sentence:\", test_sentence)\n",
    "print()\n",
    "print(\"Stemmed sentence:\", stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_preprocessor(x_list: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preprocessing function to lowercase and remove punctuation\n",
    "    of a list of string and stem each string.\n",
    "    \n",
    "    Args:\n",
    "        x_list: List of strings\n",
    "    \n",
    "    Returns:\n",
    "        List of preprocessed strings.\n",
    "    \"\"\"\n",
    "    no_punc_lower = [x.lower().translate(str.maketrans(\"\", \"\", punctuation)) for x in x_list]\n",
    "    re_word = re.compile(r\"^\\w+$\")\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    res = []\n",
    "    for sentence in no_punc_lower:\n",
    "        s = []\n",
    "        for word in word_tokenize(sentence):\n",
    "            if re_word.match(word):\n",
    "                s.append(stemmer.stem(word))\n",
    "        s = ' '.join(s)\n",
    "        res.append(s)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocess',\n",
       "                 FunctionTransformer(func=<function stem_preprocessor at 0x7f72d4ce9bd0>)),\n",
       "                ('vectorizer', CountVectorizer()), ('nb', MultinomialNB())])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocess\", FunctionTransformer(stem_preprocessor)),\n",
    "    (\"vectorizer\", CountVectorizer(lowercase=True)),\n",
    "    (\"nb\", MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Fit the pipeline with train dataset\n",
    "pipeline.fit(np.array(dataset[\"train\"][\"text\"]), np.array(dataset[\"train\"][\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.90068\n",
      "Test accuracy:  0.80972\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the prediction with train and test datasets\n",
    "print(\"Train accuracy: \", accuracy_score(pipeline.predict(np.array(dataset[\"train\"][\"text\"])), np.array(dataset[\"train\"][\"label\"])))\n",
    "print(\"Test accuracy: \", accuracy_score(pipeline.predict(np.array(dataset[\"test\"][\"text\"])), np.array(dataset[\"test\"][\"label\"])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
